{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e7074e",
   "metadata": {},
   "source": [
    "# Start Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.clean(clean_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe18b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "torch.__version__\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "user = getpass.getuser()\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d56ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_path = f\"omniverse://localhost/Users/{user}/UVA/uva_table.usd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8d7fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from omni.isaac.kit import SimulationApp    \n",
    "simulation_app = SimulationApp({\"headless\": True, \"open_usd\": usd_path,  \"livesync_usd\": usd_path}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "import logging\n",
    "import carb\n",
    "\n",
    "logging.getLogger(\"omni.hydra\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"omni.isaac.urdf\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"omni.physx.plugin\").setLevel(logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"omni.isaac.synthetic_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"omni.isaac.synthetic_utils.syntheticdata\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"omni.hydra.scene_delegate.plugin\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "l = carb.logging.LEVEL_ERROR\n",
    "carb.settings.get_settings().set(\"/log/level\", l)\n",
    "carb.settings.get_settings().set(\"/log/fileLogLevel\", l)\n",
    "carb.settings.get_settings().set(\"/log/outputStreamLevel\", l)\n",
    "\n",
    "# # This logged error is printed as it should\n",
    "# carb.log_error(\"ERROR\")\n",
    "# # This warning is printed but should not\n",
    "# carb.log_warn(\"WARNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34671d",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task.config import DATA_PATH, FEATURE_PATH\n",
    "task_type = \"Table\"\n",
    "side_choice = \"Border\"\n",
    "base_asset_id = 0\n",
    "load_nucleus = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475ee6a",
   "metadata": {},
   "source": [
    "# Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uva_env import UvaEnv\n",
    "env = UvaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e665fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task.utils import add_scene_default\n",
    "add_scene_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55eb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(env.stage.TraverseAll()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaeb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.clean()\n",
    "env.world.step(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c365e3",
   "metadata": {},
   "source": [
    "# Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task.scene import ArrangeScene\n",
    "scene = ArrangeScene(task_type, side_choice, base_asset_id = 0, traj_id = 0, load_nucleus = load_nucleus)\n",
    "env.scene = scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae909b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add base\n",
    "scene.add_base_asset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add room\n",
    "scene.add_room()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99481529",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.world.step(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize scene\n",
    "from layout.randomizer import Randomizer\n",
    "randomizer = Randomizer(load_nucleus = load_nucleus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd24034",
   "metadata": {},
   "source": [
    "# Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uv.reward import Rewarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35761844",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewarder = Rewarder(env.world)\n",
    "env.rewarder = rewarder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8bada",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from render.helper import RenderHelper\n",
    "render = RenderHelper(task_type, side_choice)\n",
    "\n",
    "render.add_task_cameras(camera_type=\"main\")\n",
    "render.add_task_cameras(camera_type=\"side\")\n",
    "render.set_cameras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "render.camera_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d05f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faba50",
   "metadata": {},
   "source": [
    "#  Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from learning.network.resnet import ResNetFeatureExtractor\n",
    "\n",
    "# feature extraction\n",
    "from learning.utils import extract_image_clip_feature_and_save, obtain_action_from_trainer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "feature_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "feature_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# replay buffer\n",
    "import json\n",
    "from learning.replay_buffer import ReplayBuffer\n",
    "from learning.config import *\n",
    "\n",
    "buffer = ReplayBuffer(max_size=2000)\n",
    "\n",
    "# trainer\n",
    "from learning.network.sac import *\n",
    "\n",
    "policy = Policy()\n",
    "\n",
    "qf1 = QFunction()\n",
    "qf2 = QFunction()\n",
    "target_qf1 = QFunction()\n",
    "target_qf2 = QFunction()\n",
    "\n",
    "from learning.sac_trainer import SACTrainer\n",
    "from learning.ddpg_trainer import ddpg_trainer\n",
    "\n",
    "trainer = SACTrainer(policy, qf1, qf2, target_qf1, target_qf2, \n",
    "     use_automatic_entropy_tuning = False, \n",
    "     policy_lr=1e-3, \n",
    "     qf_lr=1e-3,\n",
    "     target_update_period = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb08ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a93eef8c",
   "metadata": {},
   "source": [
    "# Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_network = True\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_traj = 0\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33335313",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9049e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traj config\n",
    "for traj_id in range(1):\n",
    "    print(\"total_traj: \", total_traj)\n",
    "    env.scene.traj_id = traj_id\n",
    "    \n",
    "    # base\n",
    "    if total_traj % 5 == 0:\n",
    "        asset_count = len(env.scene.base_asset_file_paths)\n",
    "        base_asset_id = random.choice([_ for _ in range(asset_count)])\n",
    "        env.scene.base_asset_id = base_asset_id    \n",
    "        env.clean(clean_base = True, clean_all = False)\n",
    "        scene.add_base_asset()\n",
    "        \n",
    "        # randomize scene\n",
    "        randomizer.randomize_house(rand = True)\n",
    "        \n",
    "    image_folder = os.path.join(DATA_PATH, task_type, side_choice, str(traj_id))\n",
    "    \n",
    "    env.world.step(render = True)\n",
    "    env.world.step(render = True)\n",
    "\n",
    "    # get images\n",
    "    env.world.render()\n",
    "    images = render.get_images()\n",
    "    render.save_rgb(images[0]['rgb'], image_folder, \"0\")\n",
    "    render.save_rgb(images[1]['rgb'], image_folder, \"0_side\")\n",
    "    \n",
    "    ## extract feature\n",
    "    \n",
    "#     extract_image_feature_and_save(images[0]['rgb'][:,:,:3], \n",
    "#         feature_extractor, os.path.join(image_folder, str(0) + \".pt\"))\n",
    "    if use_network:\n",
    "        extract_image_clip_feature_and_save(f\"{image_folder}/{0}.png\", feature_model, feature_processor, \n",
    "            f\"{image_folder}/{0}.pt\",)\n",
    "    \n",
    "\n",
    "    # trajectory\n",
    "    for step in range(5):\n",
    "        total_step += 1\n",
    "        \n",
    "        # sample an object\n",
    "        env.add_scene_obj(mode = \"random\")\n",
    "        \n",
    "        # TODO: get action from sampling\n",
    "        if not use_network or total_traj < 10 or np.random.rand() < 0.2:\n",
    "            x, y = np.tanh(np.random.randn()), np.tanh(np.random.randn())\n",
    "        else:\n",
    "            image_feature_file = f\"{image_folder}/{step}.pt\"\n",
    "            \n",
    "            object_info = env.scene.objects[-1]\n",
    "            object_type = object_info[\"type\"]\n",
    "            obj_name = object_info[\"name\"][:-4]\n",
    "            object_feature_file = os.path.join(FEATURE_PATH, object_type, obj_name + \".pt\")\n",
    "            x, y = obtain_action_from_trainer(image_feature_file, object_feature_file, trainer, \n",
    "                                              scaler=np.exp(- total_traj / 100))\n",
    "            object_info[\"use_network\"] = True\n",
    "        \n",
    "        # load the object into the scene\n",
    "        env.put_last_object((x, y)) \n",
    "        env.world.step(render=True)\n",
    "        \n",
    "        # register the object to the world for physics update\n",
    "        env.register_last_object()\n",
    "        env.world.step(render=True)\n",
    "\n",
    "        # get images\n",
    "        env.world.render()\n",
    "        images = render.get_images()\n",
    "        render.save_rgb(images[0]['rgb'], image_folder, str(step + 1))\n",
    "        render.save_rgb(images[1]['rgb'], image_folder, f\"{step + 1}_side\")\n",
    "\n",
    "        ## calculate reward\n",
    "        env.calculate_last_reward(simulation_step = 30)\n",
    "        \n",
    "        ## extract feature\n",
    "#         extract_image_feature_and_save(images[0]['rgb'][:,:,:3], \n",
    "#             feature_extractor, os.path.join(image_folder, str(step + 1) + \".pt\"))\n",
    "        if use_network:\n",
    "            extract_image_clip_feature_and_save(f\"{image_folder}/{step + 1}.png\", feature_model, feature_processor, \n",
    "                f\"{image_folder}/{step + 1}.pt\",)\n",
    "    \n",
    "\n",
    "        ## reset\n",
    "        env.world.reset()\n",
    "        env.world.step(render=True)\n",
    "        \n",
    "        ## trainer nework\n",
    "        if use_network and total_step % UPDATE_TRAINER_STEPS == 0 and total_traj > 5:\n",
    "            batch = buffer.sample_batch(batch_size = BATCH_SIZE)\n",
    "            trainer.update(batch)\n",
    "            \n",
    "            if debug and total_step % 10 == 0:\n",
    "                rewards = batch['rewards'].to(device)\n",
    "                terminals = batch['terminals'].to(device)\n",
    "                obs = batch['observations'].to(device)\n",
    "                actions = batch['actions'].to(device)\n",
    "                next_obs = batch['next_observations'].to(device)\n",
    "                obj_features = batch['object_features'].to(device)\n",
    "                \n",
    "                dist = trainer.policy(obs, obj_features)\n",
    "                pred = trainer.qf1(obs, obj_features, actions)\n",
    "                print(f\"debug {total_traj}/{total_step}\", #\"\\n dist: \", dist.mean.flatten().tolist(), dist.stddev.flatten().tolist(),\n",
    "                      \"\\n pred:\", pred.flatten().tolist(),\n",
    "                      \"\\n rewards: \", rewards.flatten().tolist())\n",
    "\n",
    "\n",
    "    # Record\n",
    "    record = env.scene.get_scene_data()\n",
    "    env.scene.save_scene_data()\n",
    "    # print(\"record: \", record)\n",
    "    \n",
    "    # Add record to buffer\n",
    "    if use_network:\n",
    "        buffer.add_scene_sample(record)\n",
    "\n",
    "    # Reset (env clean)\n",
    "    env.clean(clean_all = False)\n",
    "    env.step(render = True)\n",
    "    total_traj += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11155e93",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de607bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.clean(clean_base = True, clean_all = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images\n",
    "env.world.render()\n",
    "env.world.render()\n",
    "images = render.get_images()\n",
    "\n",
    "Image.fromarray(images[0]['rgb'], \"RGBA\").resize((300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3687f5",
   "metadata": {},
   "source": [
    "##  Debug Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d01e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from learning.replay_buffer import ReplayBuffer\n",
    "\n",
    "buffer = ReplayBuffer(max_size=1000)\n",
    "\n",
    "for i in range(1):\n",
    "    replay = json.load(open(f\"{DATA_PATH}/{task_type}/{side_choice}/{i}/scene.json\"))\n",
    "    buffer.add_scene_sample(replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec87318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(buffer.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63777cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "from learning.network.sac import *\n",
    "\n",
    "policy = Policy()\n",
    "target_policy = Policy()\n",
    "\n",
    "qf1 = QFunction()\n",
    "qf2 = QFunction()\n",
    "\n",
    "target_qf1 = QFunction()\n",
    "target_qf1.eval()\n",
    "\n",
    "target_qf2 = QFunction()\n",
    "target_qf2.eval()\n",
    "\n",
    "from learning.sac_trainer import SACTrainer\n",
    "from learning.ddpg_trainer import DDPGTrainer\n",
    "\n",
    "trainer = SACTrainer(policy, qf1, qf2, target_qf1, target_qf2, \n",
    "     use_automatic_entropy_tuning = True, \n",
    "     policy_lr=1e-3, \n",
    "     qf_lr=1e-3,\n",
    "     target_update_period = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85224ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DDPGTrainer(qf1, target_qf1, policy, target_policy,\n",
    "     policy_learning_rate=1e-3,\n",
    "     qf_learning_rate=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95f838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    batch = buffer.sample_batch(batch_size=4)\n",
    "\n",
    "    rewards = batch['rewards'].to(device)\n",
    "    terminals = batch['terminals'].to(device)\n",
    "    obs = batch['observations'].to(device)\n",
    "    actions = batch['actions'].to(device)\n",
    "    next_obs = batch['next_observations'].to(device)\n",
    "\n",
    "    obj_features = batch['object_features'].to(device)\n",
    "\n",
    "    # print(\"rewards\", rewards)\n",
    "\n",
    "    self.update(batch)\n",
    "    \n",
    "    if _ % 10 == 0:\n",
    "        dist = self.policy(obs, obj_features)\n",
    "        pred = self.qf1(obs, obj_features, actions)\n",
    "        print(_, \"\\n dist   : \", [round(x, 3) for x in dist.mean.flatten().tolist()], \n",
    "                  [round(x, 3) for x in dist.stddev.flatten().tolist()],\n",
    "              \"\\n pred   :\", [round(x, 3) for x in pred.flatten().tolist()],\n",
    "              \"\\n rewards: \", [round(x, 3) for x in rewards.flatten().tolist()])\n",
    "    \n",
    "#     # debug\n",
    "#     pred = self.qf1(obs, obj_features, actions)\n",
    "#     loss = self.qf_criterion(pred, rewards)\n",
    "    \n",
    "#     self.qf1_optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     self.qf1_optimizer.step()\n",
    "    \n",
    "#     if _ % 20 == 0:\n",
    "#         print(_, \"\\n debug loss: \", loss.item(), \"\\n pred:\", \n",
    "#               pred.flatten().tolist(), \"\\n rewards: \", rewards.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59102e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = self.policy(obs, obj_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = self.policy(obs.to(device), obj_features.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.normal_mean, dist.stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.sample(scaler = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa47896",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.normal_mean + scaler * self.normal_std * MultivariateDiagonalNormal(\n",
    "    torch.zeros(self.normal_mean.size()).to(self.device),\n",
    "    torch.ones(self.normal_std.size()).to(self.device)\n",
    ").sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9643df",
   "metadata": {},
   "source": [
    "# distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be282f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.distributions import MultivariateDiagonalNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = JustNormal(dist.mean, dist.stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.rsample_and_logprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bf0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Isaac Sim Python 3",
   "language": "python",
   "name": "isaac_sim_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
